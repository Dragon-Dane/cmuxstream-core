% !TEX root = ./xstream.tex

\section{Static evaluation}

\begin{table}[h!]
	\centering
	\begin{tabular}{lrr}
	\toprule
	\textbf{Dataset} & \textbf{Number of samples} & \textbf{Dimensionality} \\
	\midrule
	\multicolumn{3}{c}{\textit{High-dimensional Datasets}}\\
	gisette 		& 7000 & 4970	\\
	isolet 			& 7797 & 616	\\
	letter			& 7797 & 616	\\
	madelon 		& 2600 & 500	\\
	\midrule
	\multicolumn{3}{c}{\textit{Low/medium-dimensional Datasets}}\\
	cancer 			& 568      & 30 \\
	ionosphere  & 350      & 33 \\
	telescope   & 19020    & 10 \\
	indians    	& 768      & 8	\\
	\bottomrule
	\end{tabular}
	\caption{Datasets used for the static evaluation.}
	\label{table:datasets}
\end{table}

\subsection{Datasets}

The datasets used are listed in Table \ref{table:datasets} to generate the subsequent datasets used in the static evaluation. They were chosen based on (TODO: selection logic).

\subsection{Generating anomalies for static evaluation}

We used 2 different procedures to generate the datasets on which to benchmark, one for the low/medium dimensional datasets and one for the high-dimensional datasets. They are described below:
\begin{enumerate}
	\item \textit{Low/medium-dimensional datasets:} We append new columns containing Gaussian noise to the original dataset, thereby increasing its dimensionality with noisy features. The number of noisy features is quantified as a percentage of the original dataset dimensionality. If the original dataset mean and standard deviation are $\mu$ and $\sigma$, the noisy columns contained Gaussian noise with mean $\rho\mu$ and standard deviation $\rho\sigma$, where $\rho$ is the noise factor. We consider appending 100\%, 1000\%, 2000\% and 5000\% noisy features, and use $\rho \in \{0.01, 0.1, 0.2, 0.25\}$.

	\item \textit{High-dimensional datasets:} We first discard all the original anomalous points. Now for the remaining nominal points, we first choose 10\% of the features at random and mark them as important features. We further proceed with choosing 5\% of the samples at random as important samples. Now, in this sub-block, which contains only important samples and important features, we add Gaussian noise to the original data. Again, Gaussian noise is used, parameterized by the mean, and the standard deviation of the entire nominal dataset. Further this noise is scaled according to the user-specified signal to noise ratio, and added to the original sub-block of our dataset and features.
\end{enumerate}

\pagebreak

\subsection{Competing methods and hyperparameter settings}

\textbf{XStream (XS)} was used with $K=100$ projections, $C=100$ chains and depth $d=15$. We implemented and compared against the following competing methods and parameter settings:

\begin{itemize}
	\item \textbf{iForest (IF)}: We use it with recommended sample size of $256$ or the entire dataset, whichever is minimum. The \texttt{hlim} is set to $15$ to be comparable to HS-Trees, and number of components is set to $100$.

	\item \textbf{HS-Trees (HST)}:  Use it with the recommended setting of $15$ max-depth and $100$ trees.

	\item \textbf{LODA}: We set the sparsity factor to $\frac{1}{\sqrt(d)}$. We allow LODA to select best width for histograms, but fix the number of histograms to $100$.

	\item \textbf{RS-Hash (RSH)}: We use $1000$ sampling points and $100$ components.
\end{itemize}

\subsection{Baseline Results}

Table \ref{table:static-results} lists the average precision of XStream and the competing methods on the generated datasets.

A Friedman test for differences in the best-performing method across all datasets failed to reject the null hypothesis (that the difference ranking between methods is not statistically significant) with $p=2.2\times10^{-16} < 0.05$. However, note that the Friedman test only takes into account the rank (with respect to the AP) of each method on each dataset, and not the actual AP magnitude.

Hence, we also perform a Wilcoxon signed-rank test to take into account the difference in AP magnitudes between pairs of methods over all datasets. Table \ref{table:wilcoxon-all} contains $p$-values from the Wilcoxon signed-rank test. Each cell $(i,j)$ in the table contains the $p$-value of the hypothesis that the difference in AP between methods $i$ and $j$ is statistically significant. Hence, \textbf{XS} > \textbf{LODA} > \textbf{RSH} > \textbf{IF} > \textbf{HST} is statistically significant at $p=0.05$.
\begin{table}[h!]
		\centering
    \begin{tabular}{lllllll}
    \toprule
    ~        & \textbf{IF}   & \textbf{RSH}    & \textbf{LODA}      & \textbf{HST}  & \textbf{XS} \\
		\midrule
    \textbf{IF}  	 & ---       			& 0.9870    		& 0.9727   			& \textbf{0.0194}   						& 0.9989       \\
    \textbf{RSH}   & \textbf{0.0135} 	& ---       		& 0.9542   			& $\pmb{6.7540\times10^{-5}}$ & 0.9981       \\
    \textbf{LODA}  & \textbf{0.0282} 	& 0.0471    		& ---      			& 0.0003   							& 0.9999      \\
    \textbf{HST} 	 & 0.9812    			& 0.9999    		& 0.9997   			& ---         					& 1.0000       \\
    \textbf{XS}    & \textbf{0.0012} & \textbf{0.0020} & 0.0001$^{**}$ & $\pmb{2.0490\times10^{-5}}$  & ---       \\
		\bottomrule
    \end{tabular}
    \caption{$p$-values from a Wilcoxon signed-rank test on the average precisions reported on all the datasets. Bold values are significant at $p=0.05$. Each cell $(i,j)$ contains the $p$-value of the hypothesis that method $i$ has a greater AP than method $j$. It can be observed that \textbf{XS} > \textbf{LODA} > \textbf{RSH} > \textbf{IF} > \textbf{HST} is statistically significant at $p=0.05$.}
		\label{table:wilcoxon-all}
\end{table}

% \begin{table}[]
% \centering
% \begin{tabular}{l|lll}
% \hline	\hline
% \textbf{Friedman Statistic} & \textbf{Low-Dim Datasets} & \textbf{High-Dim Datasets} & \textbf{All Datasets}    \\ \hline
% p-value            & 1.155e-09        & 8.362e-10         & \textless2.2e-16 \\ \hline \hline
% \textbf{Wilcoxon Test}      & \multicolumn{3}{c}{\textbf{p-val w/ XST}}                        \\ 	\hline
% iFor               & 1.907e-06         & 0.7271         &  0.001196       \\
% RSH                & 6.676e-06         & 0.7392         & 0.002026        \\
% LODA               & 9.537e-07         & 0.1744         & 0.0001096            \\
% HSTrees            & 2.861e-06        & 0.2375          &  2.049e-05	\\	\hline
% \end{tabular}
% \caption{Friedman's test statistics among methods. And wilcoxon test p-values, indicating that XStream performs better than all the baseline methods for low-dimensional datasets and over the set of all datasets, but no method is more significant than XStream and vice-versa.}
% \label{my-label}
% \end{table}

% \begin{table}
% \centering
%     \begin{tabular}{|l|lllll|}
%     \hline
%     ~        & iForest   & RSHash    & LODA      & HS Trees  & XStream \\	\hline
%     iForest  & ~         & 0.9977    & 0.9999    & 0.4839    & 1       \\
%     RSHash   & 0.0025    & ~         & 0.9998    & 0.0752    & 1       \\
%     LODA     & 0.00013   & 0.00019   & ~         & 0.00010   & 1       \\
%     HS Trees & 0.5321    & 0.9299    & 0.999     & ~         & 1       \\
%     XStream  & 1.907e-06 & 6.676e-06 & 9.537e-07 & 2.861e-06 & ~       \\	\hline
%     \end{tabular}
%     \caption{Wilcoxon test p-value result for Low Dimension dataset. XST > LODA > RSH $\sim$ HST $\sim$ IF}
% \end{table}

% \begin{table}
% \centering
%     \begin{tabular}{|l|lllll|}
%     \hline
%     ~        & iForest   & RSHash    & LODA      & HS Trees  & XStream \\	\hline
%     iForest  & ~         & 0.7432    & 0.07528    & 0.00618    & 0.2853       \\
%     RSHash   & 0.2689    & ~         &  0.02964   & 0.0001    & 0.2729       \\
%     LODA     & 0.9299   & 0.9728   & ~         & 0.211   & 0.835      \\
%     HS Trees & 0.9944    & 0.9999    & 0.7996     & ~         & 0.7738       \\
%     XStream  & 0.7271 & 0.7392 & 0.1744 & 0.2375 & ~       \\	\hline
%     \end{tabular}
%     \caption{Wilcoxon test p-value result for High Dimension dataset. XST $\sim$ IF $\sim$ RSH >  LODA $\sim$ HST $\sim$ XST}
% \end{table}

\begin{footnotesize}
\begin{table}[p!]
    %\centering
		\begin{tabular}{lcccccc}
				\toprule
				\textbf{Dataset} & \textbf{IF} &  \textbf{LODA} & \textbf{RSH} &  \textbf{HST}  & \textbf{XS}\\
				\midrule
				\multicolumn{6}{c}{\textit{High-dimensional Datasets}}\\
%gisette& $0.423 \pm 0.011$ &  $0.436 \pm 0.008$ &  $0.417 \pm 0.005$ &  $0.432 \pm 0.017$    \\
gisette (30,0.3,1.2)  & $0.683 \pm 0.045$ &  $0.531 \pm 0.018$ &  $0.460 \pm 0.005$ &  $0.628 \pm 0.021$  & $0.528 \pm  0.009$ \\
gisette (30,0.3,10)   & $0.541 \pm 0.027$ &  $0.321 \pm 0.004$ &  $0.354 \pm 0.002$ &  $0.471 \pm 0.024$  & $0.320 \pm  0.003$   \\
gisette (30,0.3,20)   & $0.450 \pm 0.022$ &  $0.290 \pm 0.003$ &  $0.308 \pm 0.003$ &  $0.347 \pm 0.017$  & $0.292 \pm  0.003$  \\
gisette (30,0.3,30)   & $0.432 \pm 0.016$ &  $0.301 \pm 0.005$ &  $0.310 \pm 0.002$ &  $0.316 \pm 0.006$  & $0.293 \pm  0.002$  \\
\midrule
%isolet& $0.442 \pm 0.019$ &  $0.429 \pm 0.02$ &  $0.444 \pm 0.001$ &  $0.444 \pm 0.013$    \\
isolet (30.0,0.3,1.2) & $0.537 \pm 0.03$  &  $0.580 \pm 0.030$ &  $0.441 \pm 0.01$  &  $0.559 \pm 0.028$  & $0.640 \pm  0.020$   \\
isolet (30.0,0.3,10)  & $0.372 \pm 0.011$ &  $0.350 \pm 0.007$ &  $0.334 \pm 0.004$ &  $0.391 \pm 0.015$  & $0.348 \pm  0.007$  \\
isolet (30.0,0.3,20)  & $0.330 \pm 0.009$ &  $0.313 \pm 0.006$ &  $0.311 \pm 0.001$ &  $0.322 \pm 0.007$  & $0.318 \pm  0.002$   \\
isolet (30.0,0.3,30)  & $0.300 \pm 0.002$ &  $0.292 \pm 0.004$ &  $0.296 \pm 0.001$ &  $0.293 \pm 0.002$  & $0.290 \pm  0.002$  \\
\midrule
%letter-recognition& $0.475 \pm 0.012$ &  $0.46 \pm 0.011$ &  $0.453 \pm 0.003$ &  $0.471 \pm 0.009$    \\
letter (30.0,0.3,1.2) & $0.490 \pm 0.026$ &  $0.534 \pm 0.03$  &  $0.440 \pm 0.010$ &  $0.519 \pm 0.053$ & $0.597 \pm  0.020$   \\
letter (30.0,0.3,10)  & $0.374 \pm 0.015$ &  $0.337 \pm 0.003$ &  $0.339 \pm 0.008$ &  $0.375 \pm 0.007$ & $0.336 \pm  0.005$   \\
letter (30.0,0.3,20)  & $0.309 \pm 0.008$ &  $0.289 \pm 0.002$ &  $0.291 \pm 0.002$ &  $0.310 \pm 0.007$ & $0.300 \pm  0.002$   \\
letter (30.0,0.3,30)  & $0.319 \pm 0.01$  &  $0.305 \pm 0.003$ &  $0.308 \pm 0.001$ &  $0.307 \pm 0.004$ & $0.310 \pm  0.002$   \\
\midrule
%madelon& $0.501 \pm 0.008$ &  $0.515 \pm 0.014$ &  $0.507 \pm 0.003$ &  $0.511 \pm 0.01$    \\
madelon (5.0,0.05,1.2)& $0.896 \pm 0.051$ &  $1.000 \pm 0.0$   &  $0.920 \pm 0.007$ &  $0.969 \pm 0.039$ & $1.000 \pm  0.000$   \\
madelon (5.0,0.05,10) & $0.643 \pm 0.13$  &  $0.988 \pm 0.012$ &  $0.784 \pm 0.068$ &  $0.899 \pm 0.094$ & $0.957 \pm  0.023$   \\
madelon (5.0,0.05,20) & $0.240 \pm 0.093$ &  $0.112 \pm 0.027$ &  $0.160 \pm 0.039$ &  $0.377 \pm 0.143$ & $0.125 \pm  0.013$  \\
madelon (5.0,0.05,30) & $0.070 \pm 0.013$ &  $0.054 \pm 0.007$ &  $0.083 \pm 0.004$ &  $0.099 \pm 0.059$ & $0.044 \pm  0.003$   \\
\midrule
\multicolumn{6}{c}{\textit{Low/medium-dimensional Datasets}}\\
cancer (100,0.1)   & $0.661 \pm 0.036$ &  $0.839 \pm 0.024$ &  $0.727 \pm 0.010$ &  $0.702 \pm 0.026$ & $0.851 \pm  0.022$ \\
cancer (1000,0.1) & $0.476 \pm 0.043$ &  $0.686 \pm 0.047$ &  $0.394 \pm 0.007$ &  $0.531 \pm 0.047$ & $0.845 \pm  0.018$ \\
cancer (2000,0.1)  & $0.447 \pm 0.026$ &  $0.643 \pm 0.066$ &  $0.444 \pm 0.015$ &  $0.474 \pm 0.046$ & $0.828 \pm  0.021$ \\
cancer (5000,0.1)  & $0.404 \pm 0.021$ &  $0.508 \pm 0.086$ &  $0.400 \pm 0.013$ &  $0.425 \pm 0.022$ & $0.773 \pm  0.039$ \\
\midrule
%ionosphere                & $0.819 \pm 0.008$ &  $0.790 \pm 0.014$  & $0.824 \pm 0.003$ &  $0.830 \pm 0.005$ & $0.044 \pm  0.003$ \\
ionosphere (100,0.1)      & $0.756 \pm 0.031$ &  $0.771 \pm 0.013$ &  $0.773 \pm 0.005$ &  $0.725 \pm 0.018$ & $0.900 \pm  0.005$ \\
ionosphere (1000,0.1)     & $0.590 \pm 0.044$ &  $0.742 \pm 0.032$ &  $0.573 \pm 0.008$ &  $0.579 \pm 0.054$ & $0.871 \pm  0.010$ \\
ionosphere (2000,0.1)     & $0.441 \pm 0.033$ &  $0.736 \pm 0.029$ &  $0.505 \pm 0.033$ &  $0.446 \pm 0.027$ & $0.874 \pm  0.005$ \\
ionosphere (5000,0.1)     & $0.403 \pm 0.022$ &  $0.675 \pm 0.048$ &  $0.376 \pm 0.006$ &  $0.399 \pm 0.028$ & $0.797 \pm  0.012$  \\
\midrule
%magic-telescope           & $0.655 \pm 0.011$ &  $0.623 \pm 0.004$ &  $0.674 \pm 0.003$ &  $0.678 \pm 0.005$ & $0.044 \pm  0.003$ \\
telescope (100,0.1) & $0.593 \pm 0.01$  &  $0.617 \pm 0.003$ &  $0.576 \pm 0.005$ &  $0.601 \pm 0.015$ & $0.637 \pm  0.008$ \\
telescope (1000,0.1)& $0.451 \pm 0.018$ &  $0.592 \pm 0.013$ &  $0.419 \pm 0.008$ &  $0.471 \pm 0.021$ & $0.610 \pm  0.005$ \\
telescope (2000,0.1)& $0.407 \pm 0.021$ &  $0.586 \pm 0.007$ &  $0.407 \pm 0.003$ &  $0.409 \pm 0.018$ & $0.587 \pm  0.009$ \\
telescope (5000,0.1)& $0.376 \pm 0.011$ &  $0.538 \pm 0.026$ &  $0.380 \pm 0.002$ &  $0.378 \pm 0.005$ & $0.575 \pm  0.007$ \\
\midrule
%pima-indians              & $0.498 \pm 0.015$ &  $0.480 \pm 0.015$ &  $0.502 \pm 0.005$ &  $0.518 \pm 0.006$ & $0.044 \pm  0.003$ \\
indians (100,0.1)    & $0.477 \pm 0.013$ &  $0.472 \pm 0.01$  &  $0.444 \pm 0.009$ &  $0.478 \pm 0.007$ & $0.522 \pm  0.008$ \\
indians (1000,0.1)   & $0.385 \pm 0.017$ &  $0.444 \pm 0.014$ &  $0.379 \pm 0.006$ &  $0.397 \pm 0.024$ & $0.498 \pm  0.011$ \\
ndians (2000,0.1)   & $0.350 \pm 0.014$ &  $0.410 \pm 0.016$ &  $0.361 \pm 0.005$ &  $0.369 \pm 0.023$ & $0.478 \pm  0.008$ \\
indians (5000,0.1)   & $0.343 \pm 0.008$ &  $0.400 \pm 0.028$ &  $0.338 \pm 0.003$ &  $0.349 \pm 0.014$ & $0.466 \pm  0.009$ \\
				\bottomrule
		\end{tabular}
		\caption{Average precision of static methods on high-dimensional (top) and low/medium-dimensional (bottom) datasets. Mean and standard deviation reported over 10 runs. Numbers in the brackets indicate: (top) the percentage of features, fraction of samples to which noise is added, signal-to-noise ratio, and (bottom) noise column amount (as $\%$ of original dimensionality), relative noise factor.}
		\label{table:static-results}
\end{table}
\end{footnotesize}

\subsection{Time and Space Complexity}
%
%

\begin{table}[h!]
	\caption{Symbol List}
	\centering
	\begin{tabular}{lll}
		\toprule
		\textbf{Symbols} & \textbf{Description}	\\	\hline
		$N$ & Number of data points	\\	\hline
		$D$ & Number of dimensions/features	\\	\hline
		$C$ & Number of ensemble components (trees, chains, histograms, etc.)	\\	\hline
		$d$ & Max depth of trees/chains	\\	\hline
		$\psi$ & iForest/HS-Trees/RS-Hash: sampling size	\\	\hline
		$r$ & RS-Hash: Number of sampled dimensions	\\
		$b$ & LODA: number of histogram bins	\\	\hline
		$m$ & \method/RS-Hash: Number of CMS hash functions	\\
		$L$ & \method/RS-Hash:: CMS hash table size	\\	\hline
		$k$ & \method: Number of random projections	\\
		$M$ & \method: Number of subspaces	\\
		$W$ & \method: Number of windows	\\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[h!]
	\caption{Time and space complexity of the ensemble anomaly detection algorithms compared in this paper.
		In streaming case, a data point/vector arrives at a time for HS-Stream, LODA, and RS-Hash; whereas
		an update to a single feature for a data point arrives at a time.}

	\centering
	\begin{tabular}{l|lclcl}
		\toprule
		\textbf{Algorithm} & \multicolumn{2}{l}{\textbf{Time Complexity}} &&& \textbf{Space Complexity}	\\
		& \multicolumn{3}{l}{\noindent\rule[0.5ex]{0.5\linewidth}{1pt}}
		& &  \multicolumn{1}{l}{\noindent\rule[0.5ex]{0.2\linewidth}{1pt}} \\
		& Training && Scoring/Updating && \\\hline
		{\em Batch/Offline} &&& & & \\
		iForest & $O(C\psi \log \psi)$ & & $O(C \log \psi)$ &  & $O(C\psi)$	\\
		HS-Trees & $O(Cd\psi)$ & &$O(Cd)$ && $O(C2^d)$ \\
		LODA & $O(NC\sqrt{D})$  & & $O(C\sqrt{D})$  && $O(C\sqrt{D} + Cb)$ \\
		RS-Hash & $O(Crm\psi)$ & & $O(Crm)$ && $O(CLm)$\\
		\method & $O(M[NDk + Ckmd])$ & &$O(M[Dk + Ckmd])$ && $O(MCLmd)$   \\
		\hline
		{\em Streaming/Online}  & & &&& \\
		HS-Stream & \multicolumn{1}{c}{--} & & $O(Cd + C\psi)$ && $O(C2^d)$ \\
		LODA & \multicolumn{1}{c}{--} & &$O(C\sqrt{D} + Cb)$ && $O(C\sqrt{D} + Cb)$\\
		RS-Hash & \multicolumn{1}{c}{--} & & $O(Crm)$ && $O(CLm)$\\
		\method & \multicolumn{1}{c}{--} & & $O(WMCkmd)$  && $O(WMCLmd)$   \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Running time}

iForest is the fastest, RSHash is the slowest (RSHash implementation, however can be optimized)
\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/baseline/TimeAnalysis_LowDim.png}
        \caption{Low/Mid - dimensional dataset analysis}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/baseline/TimeAnalysis_HighDim.pdf}
        \caption{High dimensional dataset analysis}
    \end{subfigure}
		\hfill
    \caption{RunTimes of algorithms over datasets with different noise levels. X-axis is over different datasets with different noise levels. Y-axis is log of runtime (in seconds).}
\end{figure*}

\pagebreak
