\documentclass[11pt,onecolumn]{article}
\usepackage[letterpaper,margin=1in]{geometry}
\usepackage[document]{ragged2e}               % ragged right alignment
\usepackage{mathpazo}                         % Palatino font
\usepackage[T1]{fontenc}                      % some font encoding stuff
\usepackage{fancyhdr}                         % for nice headers and footers
\usepackage{lastpage}                         % to enable "Page 1 of X"
\usepackage{xcolor}                           % for colors
\usepackage{titlesec}                         % for section title spacing
\usepackage{url}                              % for urls
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage[font=small]{subcaption}
%\usepackage{apaparencite}
\usepackage{csquotes}
\usepackage{wrapfig,booktabs}
%\usepackage[font=small,bf]{caption}
\usepackage[style=authoryear, maxnames=1, backend=bibtex]{biblatex}
\addbibresource{xstream.bib}

\DeclareMathOperator*{\argmax}{\arg\!\max}

% set up formatting
\setlength{\parindent}{0pt}                   % don't indent paragraphs
\setlength{\parskip}{12pt}                    % space between paragraphs
\setlength{\footskip}{18pt}                   % space between last para/footer
\linespread{1.05}
\titlespacing\section{0pt}{0.25\parskip}{0.25\parskip}
\titlespacing\subsection{0pt}{0.25\parskip}{0.25\parskip}
\titlespacing\subsubsection{0pt}{\parskip}{\parskip}
%\setlength{\bibspacing}{\baselineskip}
\setlist[itemize,enumerate]{topsep=0pt}

\setlength{\columnsep}{24pt}%
\newcommand{\method}{{\sc X-stream}}

\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

% set up headers and footers
\pagestyle{fancy}
\fancyhf{}
\rfoot{\color{gray}\scriptsize{\thepage \hspace{1pt} of \pageref{LastPage}}}
\lfoot{\color{gray}\scriptsize{XStream -- Emaad Ahmed Manzoor}}
\renewcommand{\headrulewidth}{0pt}

% macros
\definecolor{blue}{HTML}{2b8cbe}
\newcommand{\note}[1]{\textcolor{blue}{#1}}
\newcommand{\R}{\mathbb{R}}

\begin{document}

\textbf{\huge{XStream}}

\section{Histogram Forests (HF)}

\begin{wraptable}{r}{5cm}
	\centering
	\small
	\begin{tabular}{cl}
		\toprule
		\multicolumn{2}{c}{\textbf{Histogram Forest Parameters}}\\
		\addlinespace[0.5em]
		$K$ & Dimension of projected data\\
		$T$ & Number of histograms\\
		$w$ & Bin width\\
		\addlinespace[0.5em]
		\multicolumn{2}{c}{\textbf{Other variables}}\\
		\addlinespace[0.5em]
		$X$ & $N\times D$ data matrix\\
		$Y$ & $N\times K$ projected data matrix\\
		$A_i$ & $K\times D$ projection matrix\\
		$b_i$ & Shift $\sim \textrm{Uniform}(0,w)$\\
		$H_i$ & Hashtable/count-min sketch\\
		\bottomrule
	\end{tabular}
	\caption{Notation.}
\end{wraptable}

\textbf{Fitting.} For each histogram $i = 1, \dots, T$:
\begin{enumerate}
	\item \textbf{Initialize the random projection matrix $A_i$.} This may be (i) sparse with $2/3$ zeros, or (ii) Gaussian random projections (which demonstrate better performance, but are computationally expensive).

	\item \textbf{Project the data.} $Y = XA_i^T$.

	\item \textbf{Sample a shift.} $b_i \sim \textrm{Uniform}(0,w)$.

	\item \textbf{Bin the data.} For each projected data point $Y_j = (y_{j1}, \dots, y_{jK})$, its bin vector is given by $\bar{Y}_j = (\floor{\frac{y_{j1} + b_i}{w}}, \dots, \floor{\frac{y_{jK} + b_i}{w}})$.

	\item \textbf{Store bin counts.} In an exact hashtable or count-min sketch $H_i$, increment the count $H_i(\bar{Y}_j)$ for $j = 1, \dots, N$.
\end{enumerate}

\textbf{Scoring.} The anomaly score of a point $X_j$ is the negative of the average of $H_i(\bar{Y}_j)$ across all histograms $i$.

\subsection{HF Results}

Methods compared in Figure 1 (on the synthetic data with noise):
\begin{itemize}
	\item \textbf{HF:} Histogram forest with varying bin width $w$, number of projected dimensions $K$ and number of histograms $T$.
	\item \textbf{IF:} iForest with 100 trees.
	\item \textbf{IF-P:} iForest with 100 trees and data projected to $k$ dimensions using sparse random projections, with varying $k$.
\end{itemize}

\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/k25T100_prcurves.pdf}
        \caption{$K=25, T=100$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/k50T100_prcurves.pdf}
        \caption{$K=50, T=100$}
    \end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/k50T200_prcurves.pdf}
        \caption{$K=50, T=200$}
    \end{subfigure}
    \caption{Precision-recall curves on synthetic data with 100 noisy dimensions.}
\end{figure*}

Observations from Figure 1:
\begin{enumerate}
	\item HF with $K = 25, T=100, w=4.0$ is on par with iForest with 100 trees on the data projected to $k = 50$ or $k = 75$ dimensions, and better than iForest for $k=25$.

	\item HF with $K = 50, T=100$ has poor performance for all $w$. This was our configuration setting for the previous histogram experiments (with some minor variations): we considered them a failure without trying a \textit{lower} value of $K$. This suggests that $K$ as a measure of data approximation quality may not be the right interpretation.

	\item HF with $K = 50, T=200$ improves performance over $T=100$ slightly, for all $w$.
\end{enumerate}

\subsection{Interpretation --- Approximate Nearest Neighbors}

\begin{wrapfigure}[23]{r}{0.35\textwidth}
    \centering
		\includegraphics[width=\linewidth]{fig/exact_nn.pdf}
		\includegraphics[width=\linewidth]{fig/exact_prcurve.pdf}
    \caption{\small(Top) Number of neighbors within radius $R$, for anomalous (red) and benign (blue) points. (Bottom) PR curves using exact nearest neighbor counts.}
\end{wrapfigure}

The binned data point in our histogram is identical to a point ``hashed'' using 2-stable distributions (\cite{datar2004locality}, 3.1), which extends LSH to solve the approximate nearest neighbors problem in $L_2$. In this method, each point $x$ is ``hashed'' using the function $h(x) = \floor{\frac{x^Ta + b}{w}}$ where $a$ is a random Gaussian vector, $b$ is a random shift and $w$ is the bin width. The probability that two points $x$ and $y$ hash to the same value is proportional to $\|x-y\|_2$.

Each point is hashed $K$ times, and points that agree on all $K$ values are considered candidate nearest neighbors. However, as $K$ grows, it becomes less likely that even nearby points agree on all $K$ values; hence, the process is repeated $T$ times, and the points agreeing on all $K$ values at in least one of the $T$ trials are considered candidate nearest neighbors.

Each configuration of $K, T$ and $w$ results in a certain \textit{probability gap} for points to be considered nearest neighbors. Specifically, this probability gap is defined by the 4-tuple $(R, p_1, cR, p_2)$:
\begin{itemize}
	\item The probability of two points with distance $\leq R$ being hashed to the same values is $\geq p_1$.
	\item The probability of two points with distance $\geq cR$ being hashed to the same values is $\leq p_2$.
\end{itemize}

Hence, our anomaly score for a point can be viewed as the negative of the approximate number of points within a sphere of some radius $R$ centered at that point. The optimal $R$ for anomaly detection on a given dataset is data-dependent (Fig. 2). Hence, the optimal $K, T, w$ are also data-dependent. This explains why a $K=25$ histogram forest may perform better than a $K=50$ one.

\subsection{Data-dependent Parameter Tuning}

\begin{wrapfigure}[14]{r}{0.3\textwidth}
    \centering
		\includegraphics[width=\linewidth]{fig/K25T100_nn.pdf}
		\includegraphics[width=\linewidth]{fig/k25T100_pr2.pdf}
    \caption{\small (Top) Distribution of approximate nearest neighbor counts via HF for different $w$ (Bottom) PR curve for HF. $K=25, T=100$.}
\end{wrapfigure}

The anomaly detection performance is sensitive to picking the right parameters $K, T$ and $w$ (equivalent to picking a radius $R$ and the LSH approximation probabilities $p_1$ and $p_2$). There have been some approaches to auto-tuning the LSH parameters based on the data. However, they were primarily designed for nearest-neighbor search and do not work for data streams. Could we design a data structure for data streams that is tailored for anomaly detection?

\begin{itemize}
	\item LSH-Forest \parencite{bawa2005lsh}. The earliest approach; this is a very simple approache that builds a trie using each bit of the $K$-element sketch. Demonstrated for the Jaccard and cosine distances (since their LSH values are binary). A theoretical analysis was recently published \parencite{andoni2017lsh}.

	\item Parameter-free LSH \parencite{ahle2017parameter}. The most recent approach, designed for Euclidean distances. This builds a ``multi-level LSH'' data structure that tries multiple values of the projection size $K$, and finds the best level for each nearest-neighbor query.
\end{itemize}

\subsection{Relationship to Chains}

\pagebreak

\section{Chains}

\begin{wraptable}{r}{5cm}
	\centering
	\small
	\begin{tabular}{cl}
		\toprule
		\multicolumn{2}{c}{\textbf{Chain Parameters}}\\
		\addlinespace[0.5em]
		$K$ & Dimension of projected data\\
		$C$ & Number of chains\\
		$D$ & Chain depth\\
		$\Delta_f$ & Initial/maximum bin width\\
		\addlinespace[0.5em]
		\multicolumn{2}{c}{\textbf{Other variables}}\\
		\addlinespace[0.5em]
		$f$ & A feature index in $1, \dots, K$\\
		$X$ & $N\times D$ data matrix\\
		$Y$ & $N\times K$ projected data matrix\\
		$A$ & $K\times D$ projection matrix\\
		$b_{if}$ & Shift $\sim \textrm{Uniform}(0,\Delta_f)$\\
		$H_{id}$ & Hashtable/count-min sketch\\
		\bottomrule
	\end{tabular}
	\caption{Notation.}
\end{wraptable}

\textbf{Pre-initialization.} This is done once, and is unchanged for all chains.
\begin{enumerate}
	\item \textbf{Initialize the random projection matrix $A$.}
	\item \textbf{Project the data.} $Y = XA^T$.
	\item \textbf{Set $\Delta_f =$} half the range of projected feature $f$.
	\item \textbf{Sample a shift $b_{if}$} for each chain $i$ and feature $f$.
\end{enumerate}

\textbf{Chain fitting.} For each chain $i = 1, \dots, C$:

\quad For each depth $d = 1, \dots, D$:
\begin{enumerate}
	\item \textbf{Sample a dimension} $f_d \in \{1, \dots K\}$ and store the dimension sampled at each depth. Let $0 \leq c(f,d) \leq d$ be the number of times dimension $f$ has been sampled in the chain until depth $d$.

	\item \textbf{Bin the data.} Let $Y_j = (y_{j1}, \dots, y_{jK})$ be a projected data point. Denote by the $Z_{jd}[f]$ the ``unfloored bin index'' of feature $f$ for point $j$ at depth $d$:
	\begin{eqnarray}
		Z_{jd}[f] = \frac{y_{jf} + b_{if}/2^{c(f,d)-1}}{\Delta_f/2^{c(f,d)-1}}\quad\forall f = 1, \dots K
	\end{eqnarray}
		At depth $d$, the bin indices $\bar{Y}_{jd}$ are logically defined as follows, for $f = 1, \dots, K$:
	\begin{eqnarray}
		\bar{Y}_{jd}[f] =
		\begin{cases}
			 0,\quad\quad\quad\quad~~~\textrm{if } c(f,d) = 0\\
			 \floor{Z_{jd}[f]}\quad\quad\textrm{ if } c(f,d) > 0
		 \end{cases}
	\end{eqnarray}
	In practice, the bins are computed recursively at each depth as follows:
	\begin{eqnarray}
		\bar{Y}_{jd}[f] =
		\begin{cases}
			0\quad\quad\quad\quad\quad\quad\quad\quad\quad~~~~\textrm{ if } c(f,d) = 0\\
			\floor{Z_{jd}[f]}\quad\quad\quad\quad\quad\quad~~~~\textrm{ if } c(f,d) = 1\\
			\floor{2 \times Z_{jd-1}[f] - \frac{b_{if}}{\Delta_f}}\quad\quad\textrm{ if } c(f,d) > 1
		\end{cases}
	\end{eqnarray}
	The recurrence is derived as follows:
	\begin{eqnarray}
		x_k = \frac{y + b/2^{k}}{\Delta/2^k}
			= \frac{y + b/2^{k-1}}{\Delta/2^{k}} - \frac{b/2^{k}}{\Delta/2^{k}}
				= 2\left(\frac{y + b/2^{k-1}}{\Delta/2^{k-1}}\right) - \frac{b}{\Delta} = 2x_{k-1} - \frac{b}{\Delta}
	\end{eqnarray}

	\item \textbf{Store bin counts.} In an exact hashtable or count-min sketch $H_{id}$, increment the count $H_{id}(\bar{Y}_j)$ for $j = 1, \dots, N$.
\end{enumerate}

\textbf{Scoring at depth $d$.} The anomaly score of a point $X_j$ at depth $d$ is $-\textrm{average}_i(H_{id}(\bar{Y}_j))$.

\pagebreak

\subsection{Results with bin-counts as anomaly scores}

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_pr_k50c10d10.pdf}
        \caption{$K=50, C=10$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_pr_k50c50d10.pdf}
        \caption{$K=50, C=50$}
    \end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_pr_k25c10d10.pdf}
        \caption{$K=25, C=10$}
    \end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_pr_k25c50d10.pdf}
        \caption{$K=25, C=50$}
    \end{subfigure}
    \caption{Precision-recall curves on synthetic data with 100 noisy dimensions, $D=10$.}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_hist_k50c10d10.pdf}
        \caption{$K=50, C=10$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_hist_k50c50d10.pdf}
        \caption{$K=50, C=50$}
    \end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_hist_k25c10d10.pdf}
        \caption{$K=25, C=10$}
    \end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_hist_k25c50d10.pdf}
        \caption{$K=25, C=50$}
    \end{subfigure}
    \caption{No. of neighbors in the same bin for anomalous (red) and benign (blue) points.}
\end{figure*}

\subsection{Results with LOCI scores as anomaly scores}

Previously, we scored each point $X_j$ using chain $i$ at depth $d$ using the number of neighbors lying in the same bin, $H_{id}(\bar{Y}_j)$. We modify this to obtain a LOCI score $L_{id}(\bar{Y}_j)$ as:
\begin{eqnarray}
	\bar{H}_{id}(\bar{Y}_j) &=& H_{id}(\bar{Y}_j) \times 2^d\\
	L_{id}(\bar{Y}_j) &=& \bar{H}_{id}(\bar{Y}_j) - \frac{1}{N-1}\sum_{k \neq j}\bar{H}_{id}(\bar{Y}_k)
\end{eqnarray}
Then the anomaly score of a point $X_j$ is $-\textrm{average}_i\left(\textrm{min}_d L_{id}(\bar{Y}_j)\right)$.

\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_anomalyscore_pr_k50c10d10.pdf}
        \caption{$K=50, C=10$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_anomalyscore_pr_k50c50d10.pdf}
        \caption{$K=50, C=50$}
    \end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_anomalyscore_pr_k25c10d10.pdf}
        \caption{$K=25, C=10$}
    \end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_anomalyscore_pr_k25c50d10.pdf}
        \caption{$K=25, C=50$}
    \end{subfigure}
    \caption{Precision-recall curves on synthetic data with 100 noisy dimensions, $D=10$.}
\end{figure*}

\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_lociscore_pr_k50c10d10.pdf}
        \caption{$K=50, C=10$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_lociscore_pr_k50c50d10.pdf}
        \caption{$K=50, C=50$}
    \end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_lociscore_pr_k25c10d10.pdf}
        \caption{$K=25, C=10$}
    \end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_lociscore_pr_k25c50d10.pdf}
        \caption{$K=25, C=50$}
    \end{subfigure}
    \caption{Precision-recall curves with LOCI scores at different depths $d$.}
\end{figure*}

\pagebreak

\subsection{Stability across runs}

\begin{table}[ht!]
    \centering
		\begin{tabular}{lll}
				\toprule
				\textbf{Chain Parameters} & \textbf{Mean AP} & \textbf{Standard Deviation}\\
				\midrule
				$K=50, C=10$ & 0.824 & 0.064\\
				$K=50, C=50$ & 0.930 & 0.001\\
				$K=25, C=10$ & 0.742 & 0.123\\
				$K=25, C=50$ & 0.901 & 0.019\\
				\bottomrule
		\end{tabular}
		\caption{Chain AP stability across 5 runs, $D=10$.}
\end{table}

\subsection{Number of unique bin IDs with depth}

\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
			\centering
			\includegraphics[width=\linewidth]{fig/chains_binids_k50c100d10.pdf}
	\end{subfigure}
	\caption{No. of unique bins for $C=100$ chains, $K=50, d=10$.}
\end{figure}

\subsection{Scores for different anomaly types}

Fig. 9 illustrates how the anomaly scores vary for different classes of points, shown in Fig. 9(a):
\begin{table}[h!]
	\centering
	\begin{tabular}{llll}
		\toprule
		\textbf{Class} & \textbf{No. of points} & \textbf{Description} & \textbf{Color}\\
		\midrule
		Class 0 & 1000 & Benign sparse points & Blue\\
		Class 1 & 2000 & Benign dense points & Orange\\
		Class 2 & 50 & Anomalous dense points & Green\\
		Class 3 & 25 & Anomalous sparse points & Red\\
		Class 4 & 6 & Locally anomalous points & Purple\\
		Class 5 & 1 & Single anomalous point & Brown\\
		\bottomrule
	\end{tabular}
\end{table}

\pagebreak

From Fig. 9(b) conforms to intuition: as the depth in the chain increases, the average number of points lying in the same bin (across chains) decreases for all classes of points. This is due to two factors: (i) increasing the number of bin dimensions, and (ii) decreasing the bin width. Point classes lying in denser regions have higher bin counts.

Fig. 9(c) shows the LOCI scores (eq. 6) at each depth $d$, scaled up by $2^d$ for visual clarity. The LOCI score of a point is simply its bin count minus the mean bin count of all other points at that depth. Hence, as $d$ increases and the bin counts of all points tend to 1, the LOCI score tends to 1.

Fig. 9(d) shows the anomaly scores. The order of anomalousness across point classes is inversely proportional to their global density. %Fig. 10 replicates Fig. 9(d) for LODA by fixing $K = 100$ (the number of projections) and varying the bin-width. The optimal bin-width (as determined by LODA) is marked in black.

\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/syndata.pdf}
        \caption{Synthetic data, colored by point class}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_type_bincounts_k50c50d10.pdf}
        \caption{Bin-counts}
    \end{subfigure}
		\\
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_type_lociscores_k50c50d10.pdf}
        \caption{LOCI scores (mean-adjusted bin-counts)}
    \end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_type_anomscores_k50c50d10.pdf}
        \caption{Anomaly scores (negative of the minimum LOCI score across all depths)}
    \end{subfigure}
    \caption{Score distributions for different point classes (see \S2.3 for details). Error bars indicate the median, 5$^{\textrm{th}}$ and 95$^{\textrm{th}}$ percentile scores across all points. Chains with $K=50, C=100, d=10$.}
\end{figure*}

% \begin{figure*}[ht!]
%     \centering
%     \begin{subfigure}[t]{0.30\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{fig/syndata.pdf}
%         \caption{Synthetic data, colored by point class}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{0.30\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{fig/chains_type_bincounts_k50c50d10.pdf}
%         \caption{Bin-counts}
%     \end{subfigure}
% 		\hfill
% 		\begin{subfigure}[t]{0.30\textwidth}
% 				\centering
% 				\includegraphics[width=\linewidth]{fig/chains_type_bincounts_k50c50d10.pdf}
% 				\caption{Bin-counts}
% 		\end{subfigure}
%     \caption{Score distributions for different point classes (see \S2.3 for details). Error bars indicate the median, 5$^{\textrm{th}}$ and 95$^{\textrm{th}}$ percentile scores across all points. Chains with $K=50, C=100, d=10$.}
% \end{figure*}

\pagebreak
\subsection{Streaming random projections}

Let the data dimensionality be $D$ and projection dimensionality be $K$. Given a density $s$, the very sparse random projections method \parencite{li2006very} generates $K$ $D$-dimensional vectors containing elements $r_{km}$ as follows ($k = 1, \dots, K, m = 1, \dots, D$):
\begin{eqnarray}
	r_{km} = \begin{cases}
		-\frac{1}{\sqrt{Ks}}\quad\textrm{ with probability }\frac{s}{2}\nonumber\\
		0\quad\textrm{ with probability }1 - s\nonumber\\
		\frac{1}{\sqrt{Ks}}\quad\textrm{ with probability }\frac{s}{2}\\
	\end{cases}
	\label{eq:randomprojections}
\end{eqnarray}
Instead of storing these vectors, we store $K$ hash functions. Each hash function $h_k$ hashes a string $w_m$ (the feature name) to a hash value $h_k(w_m)$, which serves as a plug-in replacement for $r_{km}$ without explicitly storing the random vectors.

The implementation of $h_k$ ensures that the hash values follow the probability distribution prescribed above.

The function $h_k(w_m)$ is implemented as follows:
\begin{enumerate}
	\item $b = g_k(w_m)$, where $g_k$ is drawn from a universal family and $b$ is a 32-bit integer. Having more bits in $b$ provides a more precise approximation to the desired probability distribution, but we found 32 to be sufficient.
	\item $n = b/(2^{32}-1)$ is a number between 0 and 1. With a good universal family to generate $g_k$, $n$ follows distributions $P_{g_k}(n) \sim \textrm{Uniform}(0,1)$ and $P_{w_m}(n) \sim \textrm{Uniform}(0,1)$.
	\item $n$ is used to return a number in $\{\frac{1}{\sqrt{Ks}}, 0, \frac{1}{\sqrt{Ks}}\}$ with the desired probability distribution:
		\begin{enumerate}
			\item If $0 \leq n < s/2$, return $-\frac{1}{\sqrt{Ks}}$.
			\item If $s/2 \leq n < s$, return $\frac{1}{\sqrt{Ks}}$.
			\item If $s \leq n \leq 1$, return 0.
		\end{enumerate}
\end{enumerate}

It is very important that $g_k$ be drawn from a universal (or almost-universal) string hash family with good empirical randomization properties. SipHash \parencite{aumasson2012siphash} and MurmurHash3\footnote{\url{https://github.com/aappleby/smhasher/blob/master/src/MurmurHash3.cpp}} were found to perform well, whereas multiplicative hashing does not.

Fig. \ref{fig:distortion} illustrates the empirical distortion of this streaming random projection method on the synthetic data, for both the Euclidean and cosine distances. Fig. \ref{fig:distortion-static} illustrates the distortion using the standard, static random projections.

When used as a plug-in replacement for sparse random projections in XStream on the synthetic data, there was no significant drop in average precision (mean and standard deviation reported for 5 runs): AP = $0.823 \pm 0.088$ for $K = 50, C = 50$; AP = $0.916 \pm 0.009$ for $K = 100, C = 50$; AP = $0.927 \pm 0.003$ for $K = 100, C = 100$.

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.29\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/streamhash_projection_cosine_k50.pdf}
        \caption{$K=50$, cosine.}
    \end{subfigure}
    \hfill
		\begin{subfigure}[t]{0.29\textwidth}
				\centering
				\includegraphics[width=\linewidth]{fig/streamhash_projection_cosine_k100.pdf}
				\caption{$K=100$, cosine.}
		\end{subfigure}
		\hfill
		\begin{subfigure}[t]{0.29\textwidth}
				\centering
				\includegraphics[width=\linewidth]{fig/streamhash_projection_cosine_k1000.pdf}
				\caption{$K=1000$, cosine.}
		\end{subfigure}\\
		\begin{subfigure}[t]{0.29\textwidth}
				\centering
				\includegraphics[width=\linewidth]{fig/streamhash_projection_euclidean_k50.pdf}
				\caption{$K=50$, Euclidean.}
		\end{subfigure}
		\hfill
		\begin{subfigure}[t]{0.29\textwidth}
				\centering
				\includegraphics[width=\linewidth]{fig/streamhash_projection_euclidean_k100.pdf}
				\caption{$K=100$, Euclidean.}
		\end{subfigure}
		\hfill
		\begin{subfigure}[t]{0.29\textwidth}
				\centering
				\includegraphics[width=\linewidth]{fig/streamhash_projection_euclidean_k1000.pdf}
				\caption{$K=1000$, Euclidean.}
		\end{subfigure}
    \caption{Distortion of streaming random projections for the Euclidean and cosine distance.}
		\label{fig:distortion}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.29\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/sparse_projection_cosine_k50.pdf}
        \caption{$K=50$, cosine.}
    \end{subfigure}
    \hfill
		\begin{subfigure}[t]{0.29\textwidth}
				\centering
				\includegraphics[width=\linewidth]{fig/sparse_projection_cosine_k100.pdf}
				\caption{$K=100$, cosine.}
		\end{subfigure}
		\hfill
		\begin{subfigure}[t]{0.29\textwidth}
				\centering
				\includegraphics[width=\linewidth]{fig/sparse_projection_cosine_k1000.pdf}
				\caption{$K=1000$, cosine.}
		\end{subfigure}\\
		\begin{subfigure}[t]{0.29\textwidth}
				\centering
				\includegraphics[width=\linewidth]{fig/sparse_projection_euclidean_k50.pdf}
				\caption{$K=50$, Euclidean.}
		\end{subfigure}
		\hfill
		\begin{subfigure}[t]{0.29\textwidth}
				\centering
				\includegraphics[width=\linewidth]{fig/sparse_projection_euclidean_k100.pdf}
				\caption{$K=100$, Euclidean.}
		\end{subfigure}
		\hfill
		\begin{subfigure}[t]{0.29\textwidth}
				\centering
				\includegraphics[width=\linewidth]{fig/sparse_projection_euclidean_k1000.pdf}
				\caption{$K=1000$, Euclidean.}
		\end{subfigure}
    \caption{Distortion for static random projections for the Euclidean and cosine distance.}
		\label{fig:distortion-static}
\end{figure*}

\pagebreak

\subsection{Addressing local density variation}

\begin{figure*}[ht!]
    \centering
		\begin{subfigure}[t]{0.3\textwidth}
				\centering
				\includegraphics[width=\linewidth]{fig/syndata.pdf}
				\caption{Synthetic data.}
		\end{subfigure}
		\begin{subfigure}[t]{0.3\textwidth}
				\centering
				\includegraphics[width=\linewidth]{fig/chains_type_bincounts_k50c50d5.pdf}
				\caption{Bin-counts.}
		\end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_type_neighborbincounts_k50c50d5.pdf}
        \caption{Neighbor bin-counts.}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_type_adjustedbincounts_k50c50d5.pdf}
        \caption{Adjusted bin-counts.}
    \end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_type_anomscores_k50c50d5.pdf}
        \caption{Anomaly scores.}
    \end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_type_anomscores-adj_k50c50d5.pdf}
        \caption{Adjusted anomaly scores.}
    \end{subfigure}
    \caption{Score distributions for different point classes after addressing local density variation (see \S2.3 and \S2.7). Error bars indicate the median, 5$^{\textrm{th}}$ and 95$^{\textrm{th}}$ percentile scores across all points. Chains with $K=50, C=100, d=5$.}
\end{figure*}
\begin{wrapfigure}[12]{r}{0.25\textwidth}
    \centering
		\includegraphics[width=\linewidth]{fig/grid.pdf}
    \caption{2-d grid.}
\end{wrapfigure}
Instead of adjusting the bin-count of each point $j$ with the mean bin-count of all other points, we could adjust it with the mean bin-count over \textit{neighboring bins}. This enables us to capture deviations in the \textit{local} density.

However, for a given point $j$ having a $d$-element bin-id at depth $d$, there are $3^d -1$ possible neighboring bins; enumerating all of them is infeasible, and most of them would be empty.

Instead, we could sample a fixed number of neighboring bins at random, such that the probability of sampling each neighboring bin is proportional to the likelihood of neighbors of $j$ lying in that bin This probability is proportional to the distance of the point $j$ to the edge of the neighboring bin (see $l_1$ and $l_2$ in Fig. 13: shaded neighboring bins are more likely to contain neighbors of the red point $j$). This idea was introduced as ``multi-probe LSH'' in \parencite{lv2007multi}.

Thus, for a point $j$ having a set of neighboring bins $\mathcal{N}(j)$, the adjusted LOCI score becomes:
\begin{eqnarray}
	L_{id}(\bar{Y}_j) &=& \bar{H}_{id}(\bar{Y}_j) - \frac{1}{|\mathcal{N}(j)|}\sum_{\bar{Y}_k \in \mathcal{N}(q)}\bar{H}_{id}(\bar{Y}_k)
\end{eqnarray}

\pagebreak

\subsection{Incremental maintenance of the LOCI score}

The LOCI score for chain $i$, point $j$ and depth $d$ was defined as:
\begin{eqnarray}
	L_{id}(\bar{Y}_j) &=& H_{id}(\bar{Y}_j)\times 2^d - \frac{1}{N}\sum_{k}H_{id}(\bar{Y}_k)\times 2^d
\end{eqnarray}
where $H_{id}(\bar{Y}_j)$ is the bin-count and is directly available on the arrival of every new point. Hence, the first term of the above equation is easily computable.

Let $b_{id}(c)$ be the bin-count of a bin $c$ in chain $i$ at depth $d$. Then the total bin-count $S_{id}$ (the sum of the bin-counts of all points in chain $i$ at depth $d$) is given by:
\begin{eqnarray}
	S_{id} = \sum_{k}H_{id}(\bar{Y}_k) = \sum_{c \in \textrm{bins in chain}(i,d)} b_{id}(c)^2
\end{eqnarray}
When a new point arrives, it falls into some cell $c'$, and its bin-count is incremented to $b_{id}(c') + 1$. This results in the following change to the total bin-count:
\begin{eqnarray}
	S'_{id} &=& \sum_{c \neq c'} b_{id}(c)^2 + (b_{id}(c') + 1)^2\\
					&=& \sum_{c} b_{id}(c)^2 + 2b_{id}(c') + 1\\
					&=& S_{id} + 2b_{id}(c') + 1
\end{eqnarray}
where $b_{id}(c)$ was the bin-count before the point was added, and is easily computable. This equation, along with tracking the number of points $N$, can thus be used to incrementally maintain the second term of the LOCI score.

\subsection{Dynamic chains with windows}

\pagebreak

\subsection{Streaming evaluation}

\begin{table}[ht!]
	\centering
	\begin{tabular}{llllll}
	\toprule
	\textbf{Dataset} & \textbf{Feature} & \textbf{Dynamic} & \textbf{Input size} 		  & \textbf{Dimensionality} & \textbf{Anomaly}\\
									 & \textbf{space}   & \textbf{updates} & \textbf{(incl. updates)} & \textbf{(maximum)} 			& \textbf{rate}\\
	\midrule
	network-intrusion& Fixed 	  		    & No				 & 720K    & 34   & 00.87\%\\
 	spam-sms 			 	 & Evolving 			  & No				 & 5574 	 & 8442 & 13.40\%\\
 	spam-url 			 	 & Evolving 				& No				 & 3.2M    & 2.4M & 33.00\%\\
	streamspot-flash & Evolving 				& Yes				 & 111M    & ?    & 20.00\%\\
	streamspot-java  & Evolving 				& Yes				 & ?       & ?    & 20.00\%\\
	\bottomrule
	\end{tabular}
	\caption{Datasets used for the streaming evaluation.}
	\label{table:datasets-streaming}
\end{table}

\textbf{Baselines}

\begin{table}
\centering
    \begin{tabular}{l|l|c}
    \toprule
    Methods  & Window / Continuous & Parameters                                           \\	\hline
    HS-Trees & Window              & Max Depth=15, Num Trees=100, Window Size = 256       \\
    RS-Hash  & Continuous          & Decay Rate =  0.015, Initial Sample Size = 25\% \\
    LODA     & Window              & Two-Window Histogram, WindowSize = 256               \\
    \bottomrule
    \end{tabular}
    \caption{Baseline Methods and the parameters chosen based on best performance}
\end{table}

\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/HttpSmtpContinuous_ap_over_time.pdf}
        \caption{network-intrusion}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/SpamSMS_ap_over_time.pdf}
        \caption{spam-sms}
    \end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/ap_over_time_placeholder.pdf}
        \caption{spam-url}
    \end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/ap_over_time_placeholder.pdf}
        \caption{streamspot-flash}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/ap_over_time_placeholder.pdf}
        \caption{?}
    \end{subfigure}
		\begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/ap_over_time_placeholder.pdf}
        \caption{?}
    \end{subfigure}
		\begin{subfigure}[t]{0.24\textwidth}
				\centering
				\includegraphics[width=\linewidth]{fig/xs_c_depths_init_samples_http.pdf}
				\caption{network-intrusion}
		\end{subfigure}
		\begin{subfigure}[t]{0.24\textwidth}
				\centering
				\includegraphics[width=\linewidth]{fig/xs_c_depths_init_samples_sms.pdf}
				\caption{spam-sms}
		\end{subfigure}
		\caption{(a)-(e) Average precision over time for XS-C, XS-W and baseline methods for 5 datasets. (f)-(h) Effect of varying $d$ and initial sample size $N$ on XS-C.}
\end{figure*}


\textbf{Observations.}
\begin{enumerate}
	\item There may be some sensitivity to the depth $d$ for XS-C. This reduces as $C$ is increased.
	\item Baseline method implementations have been verified to give the right ROC-AUC numbers on the datasets they originally reported on.
	\item High ROC-AUC as reported by baseline methods is not correlated with high AP (see \url{http://www.dbs.ifi.lmu.de/research/outlier-evaluation/DAMI/literature/KDDCup99/} for an example on the network-intrusion dataset). In fact, AP is a more sensitive (and better) metric than ROC-AUC.
	\item RSH performs poorly on network-intrusion due to clustered anomalies. On shuffling the dataset, the AP improves(see Fig:~\ref{fig:rsh_shuffle})
\end{enumerate}

\textbf{PR-AUC (AP) vs. ROC-AUC.} PR curves do not care about ``true negatives'', which do not show up in the formulae for precision or recall. Hence, there is no advantage in labelling non-anomalies correctly: this is desired, since having a large number of non-anomalies (as is usually the case) reduces the loss in AUC due to incorrectly labelling anomalies. In fact, a curve that dominates in PR-space will also dominate in ROC-space, but the converse is not true \parencite{davis2006relationship}.

\begin{figure}
\centering
	\includegraphics[scale=0.6]{fig/shuffled_ap_over_time.pdf}
	\caption{RS-Hash performance improves on shuffled version of HTTP dataset.}
	\label{fig:rsh_shuffle}
\end{figure}


\input{static}

\input{dynamic}

\printbibliography
\end{document}
