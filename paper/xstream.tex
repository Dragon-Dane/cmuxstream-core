\documentclass[11pt,onecolumn]{article}
\usepackage[letterpaper,margin=1in]{geometry}
\usepackage[document]{ragged2e}               % ragged right alignment
\usepackage{mathpazo}                         % Palatino font
\usepackage[T1]{fontenc}                      % some font encoding stuff
\usepackage{fancyhdr}                         % for nice headers and footers
\usepackage{lastpage}                         % to enable "Page 1 of X"
\usepackage{xcolor}                           % for colors
\usepackage{titlesec}                         % for section title spacing
\usepackage{url}                              % for urls
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage[font=small]{subcaption}
%\usepackage{apaparencite}
\usepackage{csquotes}
\usepackage{wrapfig,booktabs}
%\usepackage[font=small,bf]{caption}
\usepackage[style=authoryear, maxnames=1, backend=bibtex]{biblatex}
\addbibresource{xstream.bib}

\DeclareMathOperator*{\argmax}{\arg\!\max}

% set up formatting
\setlength{\parindent}{0pt}                   % don't indent paragraphs
\setlength{\parskip}{12pt}                    % space between paragraphs
\setlength{\footskip}{18pt}                   % space between last para/footer
\linespread{1.05}
\titlespacing\section{0pt}{0.25\parskip}{0.25\parskip}
\titlespacing\subsection{0pt}{0.25\parskip}{0.25\parskip}
\titlespacing\subsubsection{0pt}{\parskip}{\parskip}
%\setlength{\bibspacing}{\baselineskip}
\setlist[itemize,enumerate]{topsep=0pt}

\setlength{\columnsep}{24pt}%
\newcommand{\method}{{\sc X-stream}}

\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

% set up headers and footers
\pagestyle{fancy}
\fancyhf{}
\rfoot{\color{gray}\scriptsize{\thepage \hspace{1pt} of \pageref{LastPage}}}
\lfoot{\color{gray}\scriptsize{XStream -- Emaad Ahmed Manzoor}}
\renewcommand{\headrulewidth}{0pt}

% macros
\definecolor{blue}{HTML}{2b8cbe}
\newcommand{\note}[1]{\textcolor{blue}{#1}}
\newcommand{\R}{\mathbb{R}}

\begin{document}

\textbf{\huge{XStream}}

\section{Histogram Forests (HF)}

\begin{wraptable}{r}{5cm}
	\centering
	\small
	\begin{tabular}{cl}
		\toprule
		\multicolumn{2}{c}{\textbf{Histogram Forest Parameters}}\\
		\addlinespace[0.5em]
		$K$ & Dimension of projected data\\
		$T$ & Number of histograms\\
		$w$ & Bin width\\
		\addlinespace[0.5em]
		\multicolumn{2}{c}{\textbf{Other variables}}\\
		\addlinespace[0.5em]
		$X$ & $N\times D$ data matrix\\
		$Y$ & $N\times K$ projected data matrix\\
		$A_i$ & $K\times D$ projection matrix\\
		$b_i$ & Shift $\sim \textrm{Uniform}(0,w)$\\
		$H_i$ & Hashtable/count-min sketch\\
		\bottomrule
	\end{tabular}
	\caption{Notation.}
\end{wraptable}

\textbf{Fitting.} For each histogram $i = 1, \dots, T$:
\begin{enumerate}
	\item \textbf{Initialize the random projection matrix $A_i$.} This may be (i) sparse with $2/3$ zeros, or (ii) Gaussian random projections (which demonstrate better performance, but are computationally expensive).

	\item \textbf{Project the data.} $Y = XA_i^T$.

	\item \textbf{Sample a shift.} $b_i \sim \textrm{Uniform}(0,w)$.

	\item \textbf{Bin the data.} For each projected data point $Y_j = (y_{j1}, \dots, y_{jK})$, its bin vector is given by $\bar{Y}_j = (\floor{\frac{y_{j1} + b_i}{w}}, \dots, \floor{\frac{y_{jK} + b_i}{w}})$.

	\item \textbf{Store bin counts.} In an exact hashtable or count-min sketch $H_i$, increment the count $H_i(\bar{Y}_j)$ for $j = 1, \dots, N$.
\end{enumerate}

\textbf{Scoring.} The anomaly score of a point $X_j$ is the negative of the average of $H_i(\bar{Y}_j)$ across all histograms $i$.

\subsection{HF Results}

Methods compared in Figure 1 (on the synthetic data with noise):
\begin{itemize}
	\item \textbf{HF:} Histogram forest with varying bin width $w$, number of projected dimensions $K$ and number of histograms $T$.
	\item \textbf{IF:} iForest with 100 trees.
	\item \textbf{IF-P:} iForest with 100 trees and data projected to $k$ dimensions using sparse random projections, with varying $k$.
\end{itemize}

\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/k25T100_prcurves.pdf}
        \caption{$K=25, T=100$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/k50T100_prcurves.pdf}
        \caption{$K=50, T=100$}
    \end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/k50T200_prcurves.pdf}
        \caption{$K=50, T=200$}
    \end{subfigure}
    \caption{Precision-recall curves on synthetic data with 100 noisy dimensions.}
\end{figure*}

Observations from Figure 1:
\begin{enumerate}
	\item HF with $K = 25, T=100, w=4.0$ is on par with iForest with 100 trees on the data projected to $k = 50$ or $k = 75$ dimensions, and better than iForest for $k=25$.

	\item HF with $K = 50, T=100$ has poor performance for all $w$. This was our configuration setting for the previous histogram experiments (with some minor variations): we considered them a failure without trying a \textit{lower} value of $K$. This suggests that $K$ as a measure of data approximation quality may not be the right interpretation.

	\item HF with $K = 50, T=200$ improves performance over $T=100$ slightly, for all $w$.
\end{enumerate}

\subsection{Interpretation --- Approximate Nearest Neighbors}

\begin{wrapfigure}[23]{r}{0.35\textwidth}
    \centering
		\includegraphics[width=\linewidth]{fig/exact_nn.pdf}
		\includegraphics[width=\linewidth]{fig/exact_prcurve.pdf}
    \caption{\small(Top) Number of neighbors within radius $R$, for anomalous (red) and benign (blue) points. (Bottom) PR curves using exact nearest neighbor counts.}
\end{wrapfigure}

The binned data point in our histogram is identical to a point ``hashed'' using 2-stable distributions (\cite{datar2004locality}, 3.1), which extends LSH to solve the approximate nearest neighbors problem in $L_2$. In this method, each point $x$ is ``hashed'' using the function $h(x) = \floor{\frac{x^Ta + b}{w}}$ where $a$ is a random Gaussian vector, $b$ is a random shift and $w$ is the bin width. The probability that two points $x$ and $y$ hash to the same value is proportional to $\|x-y\|_2$.

Each point is hashed $K$ times, and points that agree on all $K$ values are considered candidate nearest neighbors. However, as $K$ grows, it becomes less likely that even nearby points agree on all $K$ values; hence, the process is repeated $T$ times, and the points agreeing on all $K$ values at in least one of the $T$ trials are considered candidate nearest neighbors.

Each configuration of $K, T$ and $w$ results in a certain \textit{probability gap} for points to be considered nearest neighbors. Specifically, this probability gap is defined by the 4-tuple $(R, p_1, cR, p_2)$:
\begin{itemize}
	\item The probability of two points with distance $\leq R$ being hashed to the same values is $\geq p_1$.
	\item The probability of two points with distance $\geq cR$ being hashed to the same values is $\leq p_2$.
\end{itemize}

Hence, our anomaly score for a point can be viewed as the negative of the approximate number of points within a sphere of some radius $R$ centered at that point. The optimal $R$ for anomaly detection on a given dataset is data-dependent (Fig. 2). Hence, the optimal $K, T, w$ are also data-dependent. This explains why a $K=25$ histogram forest may perform better than a $K=50$ one.

\subsection{Data-dependent Parameter Tuning}

\begin{wrapfigure}[14]{r}{0.3\textwidth}
    \centering
		\includegraphics[width=\linewidth]{fig/K25T100_nn.pdf}
		\includegraphics[width=\linewidth]{fig/k25T100_pr2.pdf}
    \caption{\small (Top) Distribution of approximate nearest neighbor counts via HF for different $w$ (Bottom) PR curve for HF. $K=25, T=100$.}
\end{wrapfigure}

The anomaly detection performance is sensitive to picking the right parameters $K, T$ and $w$ (equivalent to picking a radius $R$ and the LSH approximation probabilities $p_1$ and $p_2$). There have been some approaches to auto-tuning the LSH parameters based on the data. However, they were primarily designed for nearest-neighbor search and do not work for data streams. Could we design a data structure for data streams that is tailored for anomaly detection?

\begin{itemize}
	\item LSH-Forest \parencite{bawa2005lsh}. The earliest approach; this is a very simple approache that builds a trie using each bit of the $K$-element sketch. Demonstrated for the Jaccard and cosine distances (since their LSH values are binary). A theoretical analysis was recently published \parencite{andoni2017lsh}.

	\item Parameter-free LSH \parencite{ahle2017parameter}. The most recent approach, designed for Euclidean distances. This builds a ``multi-level LSH'' data structure that tries multiple values of the projection size $K$, and finds the best level for each nearest-neighbor query.
\end{itemize}

\subsection{Relationship to Chains}

\pagebreak

\section{Chains}

\begin{wraptable}{r}{5cm}
	\centering
	\small
	\begin{tabular}{cl}
		\toprule
		\multicolumn{2}{c}{\textbf{Chain Parameters}}\\
		\addlinespace[0.5em]
		$K$ & Dimension of projected data\\
		$C$ & Number of chains\\
		$D$ & Chain depth\\
		$\Delta_f$ & Initial/maximum bin width\\
		\addlinespace[0.5em]
		\multicolumn{2}{c}{\textbf{Other variables}}\\
		\addlinespace[0.5em]
		$f$ & A feature index in $1, \dots, K$\\
		$X$ & $N\times D$ data matrix\\
		$Y$ & $N\times K$ projected data matrix\\
		$A$ & $K\times D$ projection matrix\\
		$b_{if}$ & Shift $\sim \textrm{Uniform}(0,\Delta_f)$\\
		$H_{id}$ & Hashtable/count-min sketch\\
		\bottomrule
	\end{tabular}
	\caption{Notation.}
\end{wraptable}

\textbf{Pre-initialization.} This is done once, and is unchanged for all chains.
\begin{enumerate}
	\item \textbf{Initialize the random projection matrix $A$.}
	\item \textbf{Project the data.} $Y = XA^T$.
	\item \textbf{Set $\Delta_f =$} half the range of projected feature $f$.
	\item \textbf{Sample a shift $b_{if}$} for each chain $i$ and feature $f$.
\end{enumerate}

\textbf{Chain fitting.} For each chain $i = 1, \dots, C$:

\quad For each depth $d = 1, \dots, D$:
\begin{enumerate}
	\item \textbf{Sample a dimension} $f_d \in \{1, \dots K\}$ and store the dimension sampled at each depth. Let $0 \leq c(f,d) \leq d$ be the number of times dimension $f$ has been sampled in the chain until depth $d$.

	\item \textbf{Bin the data.} Let $Y_j = (y_{j1}, \dots, y_{jK})$ be a projected data point. Denote by the $Z_{jd}[f]$ the ``unfloored bin index'' of feature $f$ for point $j$ at depth $d$:
	\begin{eqnarray}
		Z_{jd}[f] = \frac{y_{jf} + b_{if}/2^{c(f,d)-1}}{\Delta_f/2^{c(f,d)-1}}\quad\forall f = 1, \dots K
	\end{eqnarray}
		At depth $d$, the bin indices $\bar{Y}_{jd}$ are logically defined as follows, for $f = 1, \dots, K$:
	\begin{eqnarray}
		\bar{Y}_{jd}[f] =
		\begin{cases}
			 0,\quad\quad\quad\quad~~~\textrm{if } c(f,d) = 0\\
			 \floor{Z_{jd}[f]}\quad\quad\textrm{ if } c(f,d) > 0
		 \end{cases}
	\end{eqnarray}
	In practice, the bins are computed recursively at each depth as follows:
	\begin{eqnarray}
		\bar{Y}_{jd}[f] =
		\begin{cases}
			0\quad\quad\quad\quad\quad\quad\quad\quad\quad~~~~\textrm{ if } c(f,d) = 0\\
			\floor{Z_{jd}[f]}\quad\quad\quad\quad\quad\quad~~~~\textrm{ if } c(f,d) = 1\\
			\floor{2 \times Z_{jd-1}[f] - \frac{b_{if}}{\Delta_f}}\quad\quad\textrm{ if } c(f,d) > 1
		\end{cases}
	\end{eqnarray}
	The recurrence is derived as follows:
	\begin{eqnarray}
		x_k = \frac{y + b/2^{k}}{\Delta/2^k}
			= \frac{y + b/2^{k-1}}{\Delta/2^{k}} - \frac{b/2^{k}}{\Delta/2^{k}}
				= 2\left(\frac{y + b/2^{k-1}}{\Delta/2^{k-1}}\right) - \frac{b}{\Delta} = 2x_{k-1} - \frac{b}{\Delta}
	\end{eqnarray}

	\item \textbf{Store bin counts.} In an exact hashtable or count-min sketch $H_{id}$, increment the count $H_{id}(\bar{Y}_j)$ for $j = 1, \dots, N$.
\end{enumerate}

\textbf{Scoring at depth $d$.} The anomaly score of a point $X_j$ at depth $d$ is $-\textrm{average}_i(H_{id}(\bar{Y}_j))$.

\pagebreak

\subsection{Results with bin-counts as anomaly scores}

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_pr_k50c10d10.pdf}
        \caption{$K=50, C=10$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_pr_k50c50d10.pdf}
        \caption{$K=50, C=50$}
    \end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_pr_k25c10d10.pdf}
        \caption{$K=25, C=10$}
    \end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_pr_k25c50d10.pdf}
        \caption{$K=25, C=50$}
    \end{subfigure}
    \caption{Precision-recall curves on synthetic data with 100 noisy dimensions, $D=10$.}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_hist_k50c10d10.pdf}
        \caption{$K=50, C=10$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_hist_k50c50d10.pdf}
        \caption{$K=50, C=50$}
    \end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_hist_k25c10d10.pdf}
        \caption{$K=25, C=10$}
    \end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_hist_k25c50d10.pdf}
        \caption{$K=25, C=50$}
    \end{subfigure}
    \caption{No. of neighbors in the same bin for anomalous (red) and benign (blue) points.}
\end{figure*}

\subsection{Results with LOCI scores as anomaly scores}

Previously, we scored each point $X_j$ using chain $i$ at depth $d$ using the number of neighbors lying in the same bin, $H_{id}(\bar{Y}_j)$. We modify this to obtain a LOCI score $L_{id}(\bar{Y}_j)$ as:
\begin{eqnarray}
	\bar{H}_{id}(\bar{Y}_j) &=& H_{id}(\bar{Y}_j) \times 2^d\\
	L_{id}(\bar{Y}_j) &=& \bar{H}_{id}(\bar{Y}_j) - \frac{1}{N-1}\sum_{k \neq j}\bar{H}_{id}(\bar{Y}_k)
\end{eqnarray}
Then the anomaly score of a point $X_j$ is $-\textrm{average}_i\left(\textrm{min}_d L_{id}(\bar{Y}_j)\right)$.

\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_anomalyscore_pr_k50c10d10.pdf}
        \caption{$K=50, C=10$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_anomalyscore_pr_k50c50d10.pdf}
        \caption{$K=50, C=50$}
    \end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_anomalyscore_pr_k25c10d10.pdf}
        \caption{$K=25, C=10$}
    \end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_anomalyscore_pr_k25c50d10.pdf}
        \caption{$K=25, C=50$}
    \end{subfigure}
    \caption{Precision-recall curves on synthetic data with 100 noisy dimensions, $D=10$.}
\end{figure*}

\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_lociscore_pr_k50c10d10.pdf}
        \caption{$K=50, C=10$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_lociscore_pr_k50c50d10.pdf}
        \caption{$K=50, C=50$}
    \end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_lociscore_pr_k25c10d10.pdf}
        \caption{$K=25, C=10$}
    \end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_lociscore_pr_k25c50d10.pdf}
        \caption{$K=25, C=50$}
    \end{subfigure}
    \caption{Precision-recall curves with LOCI scores at different depths $d$.}
\end{figure*}

\pagebreak

\subsection{Stability across runs}

\begin{table}[ht!]
    \centering
		\begin{tabular}{lll}
				\toprule
				\textbf{Chain Parameters} & \textbf{Mean AP} & \textbf{Standard Deviation}\\
				\midrule
				$K=50, C=10$ & 0.824 & 0.064\\
				$K=50, C=50$ & 0.930 & 0.001\\
				$K=25, C=10$ & 0.742 & 0.123\\
				$K=25, C=50$ & 0.901 & 0.019\\
				\bottomrule
		\end{tabular}
		\caption{Chain AP stability across 5 runs, $D=10$.}
\end{table}

\subsection{Number of unique bin IDs with depth}

\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
			\centering
			\includegraphics[width=\linewidth]{fig/chains_binids_k50c100d10.pdf}
	\end{subfigure}
	\caption{No. of unique bins for $C=100$ chains, $K=50, d=10$.}
\end{figure}

\subsection{Scores for different anomaly types}

Fig. 9 illustrates how the anomaly scores vary for different classes of points, shown in Fig. 9(a):
\begin{table}[h!]
	\centering
	\begin{tabular}{llll}
		\toprule
		\textbf{Class} & \textbf{No. of points} & \textbf{Description} & \textbf{Color}\\
		\midrule
		Class 0 & 1000 & Benign sparse points & Blue\\
		Class 1 & 2000 & Benign dense points & Orange\\
		Class 2 & 50 & Anomalous dense points & Green\\
		Class 3 & 25 & Anomalous sparse points & Red\\
		Class 4 & 6 & Locally anomalous points & Purple\\
		Class 5 & 1 & Single anomalous point & Brown\\
		\bottomrule
	\end{tabular}
\end{table}

\pagebreak

From Fig. 9(b) conforms to intuition: as the depth in the chain increases, the average number of points lying in the same bin (across chains) decreases for all classes of points. This is due to two factors: (i) increasing the number of bin dimensions, and (ii) decreasing the bin width. Point classes lying in denser regions have higher bin counts.

Fig. 9(c) shows the LOCI scores (eq. 6) at each depth $d$, scaled up by $2^d$ for visual clarity. The LOCI score of a point is simply its bin count minus the mean bin count of all other points at that depth. Hence, as $d$ increases and the bin counts of all points tend to 1, the LOCI score tends to 1.

Fig. 9(d) shows the anomaly scores. The order of anomalousness across point classes is inversely proportional to their global density. %Fig. 10 replicates Fig. 9(d) for LODA by fixing $K = 100$ (the number of projections) and varying the bin-width. The optimal bin-width (as determined by LODA) is marked in black.

\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/syndata.pdf}
        \caption{Synthetic data, colored by point class}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_type_bincounts_k50c50d10.pdf}
        \caption{Bin-counts}
    \end{subfigure}
		\\
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_type_lociscores_k50c50d10.pdf}
        \caption{LOCI scores (mean-adjusted bin-counts)}
    \end{subfigure}
		\hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/chains_type_anomscores_k50c50d10.pdf}
        \caption{Anomaly scores (negative of the minimum LOCI score across all depths)}
    \end{subfigure}
    \caption{Score distributions for different point classes (see \S2.3 for details). Error bars indicate the median, 5$^{\textrm{th}}$ and 95$^{\textrm{th}}$ percentile scores across all points. Chains with $K=50, C=100, d=10$.}
\end{figure*}

% \begin{figure*}[ht!]
%     \centering
%     \begin{subfigure}[t]{0.30\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{fig/syndata.pdf}
%         \caption{Synthetic data, colored by point class}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{0.30\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{fig/chains_type_bincounts_k50c50d10.pdf}
%         \caption{Bin-counts}
%     \end{subfigure}
% 		\hfill
% 		\begin{subfigure}[t]{0.30\textwidth}
% 				\centering
% 				\includegraphics[width=\linewidth]{fig/chains_type_bincounts_k50c50d10.pdf}
% 				\caption{Bin-counts}
% 		\end{subfigure}
%     \caption{Score distributions for different point classes (see \S2.3 for details). Error bars indicate the median, 5$^{\textrm{th}}$ and 95$^{\textrm{th}}$ percentile scores across all points. Chains with $K=50, C=100, d=10$.}
% \end{figure*}

\subsection{Results on other high-dimensional datasets}

See \S3 for details on the datasets, and results with baseline methods.

\begin{table}[ht!]
    \centering
		\begin{tabular}{lll}
				\toprule
				\textbf{Dataset} & \textbf{AP} & \textbf{AUC}\\
				\midrule
				\multicolumn{3}{c}{\textit{High-dimensional Datasets}}\\
				Madelone & $0.5174 \pm 0.0056$ & $0.5127 \pm 0.0078$\\
				Madelone-Noisy (5.0, 0.1, 10.0) & $0.8271 \pm 0.0360$ & $0.9749 \pm 0.0059$\\
				Madelone-Noisy (5.0, 0.1, 20.0) & $0.1485 \pm 0.0178$ & $0.6587 \pm 0.0215$\\
				Madelone-Noisy (5.0, 0.1, 50.0) & $0.1161 \pm 0.0044$ & $0.5002 \pm 0.0106$\\
				\midrule
				Letter-Recognition & $0.4688 \pm 0.0087$ & $0.5274 \pm 0.0044$\\
				Letter-Recognition-Noisy (5.0, 0.1, 10.0) & $0.1019 \pm 0.0018$ & $0.5005 \pm 0.0046$\\
				Letter-Recognition-Noisy (5.0, 0.1, 20.0) & $0.1088 \pm 0.0022$ & $0.5046 \pm 0.0056$\\
				Letter-Recognition-Noisy (5.0, 0.1, 50.0) & $0.1068 \pm 0.0016$ & $0.5010 \pm 0.0062$\\
				\midrule
				Gisette & $0.4633 \pm 0.0081$ & $0.4598 \pm 0.0137$\\
				Gisette-Noisy (5.0, 0.1, 10.0) & $0.0934 \pm 0.0012$ & $0.4869 \pm 0.0069	$\\
				Gisette-Noisy (5.0, 0.1, 20.0) & $0.1090 \pm 0.0025$ & $0.5106 \pm 0.0082$\\
				Gisette-Noisy (5.0, 0.1, 50.0) & $0.0925 \pm 0.0013$ & $0.4933 \pm 0.0056$\\
				\midrule
				Isolette & $0.4402 \pm 0.0131$ & $0.5145 \pm 0.0109$\\
				Isolette-Noisy (5.0, 0.1, 10.0) & $0.1098 \pm 0.0023$ & $0.5338 \pm 0.0042$\\
				Isolette-Noisy (5.0, 0.1, 20.0) & $0.0895 \pm 0.0017$ & $0.4894 \pm 0.0064$\\
				Isolette-Noisy (5.0, 0.1, 50.0) & $0.0968 \pm 0.0018$ & $0.4929 \pm 0.0041$\\
				\bottomrule
		\end{tabular}
		\caption{AP/AUC over 10 runs, $K=50, C=100, D=10$.}
\end{table}

\textbf{Note on results on Madelone-Noisy (5.0, 0.1, 10.0) vs. LODA.} LODA selects the optimal number histograms and bins (equivalently, bin width) based on the data. In the streaming variant, a sample of the data is used to select these hyperparameters. Once selected, they remain fixed for the entire duration of the LODA run.

XStream, on the other hand, relies on ensembling instead of finding the ``right'' bin-width. Increasing $K$ for XStream to the optimal selected by LODA ($K = 125$) brings the AP up to $\sim0.90$.

There is an additional factor that makes LODA's performance superior in the static case: LODA uses sparse \textit{Gaussian} random projections, which are a better approximation than the sparse \textit{binary} random projections used by XStream.

Note that increasing $K$ up to $200$ makes the AP comparable to that of LODA.

\pagebreak

\subsection{Results on other low/medium dimensional datasets}

See \S3 for details on the datasets, and results with baseline methods.

\begin{table}[ht!]
    \centering
		\begin{tabular}{lll}
				\toprule
				\textbf{Dataset} & \textbf{AP} & \textbf{AUC}\\
				\midrule
				\multicolumn{3}{c}{\textit{Low/medium-dimensional Datasets}}\\
				Breast-Cancer-Wisconsin & $0.9007 \pm 0.0083$ & $0.9231 \pm 0.0074$\\
				Breast-Cancer-Wisconsin-Noisy (100, 0.1) & $0.8514 \pm 0.0180$ & $0.8625 \pm 0.0186$\\
				Breast-Cancer-Wisconsin-Noisy (1000, 0.1) & $0.8514 \pm 0.0180$ & $0.8625 \pm 0.0186$\\
				Breast-Cancer-Wisconsin-Noisy (2000, 0.1) & $0.8514 \pm 0.0180$ & $0.8625 \pm 0.0186$\\
				Breast-Cancer-Wisconsin-Noisy (5000, 0.1) & $0.8514 \pm 0.0180$ & $0.8625 \pm 0.0186$\\
				\midrule
				Ionosphere & $0.8425 \pm 0.0056$ & $0.8855 \pm 0.0046$\\
				\midrule
				Magic-Telescope & $0.4633 \pm 0.0081$ & $0.4598 \pm 0.0137$\\
				\midrule
				Pima-Indians & $0.5458 \pm 0.0075$ & $0.7041 \pm 0.0096$\\
				\bottomrule
		\end{tabular}
		\caption{AP/AUC over 10 runs, $K=50, C=100, D=10$.}
\end{table}

\input{static}

\printbibliography
\end{document}
